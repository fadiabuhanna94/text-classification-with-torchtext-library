{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MWcivI7eQrG-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetClassifier(nn.ModuleList):\n",
        "\n",
        "\tdef __init__(self, args):\n",
        "\t\tsuper(TweetClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\t\tself.hidden_dim = args.hidden_dim\n",
        "\t\tself.LSTM_layers = args.lstm_layers\n",
        "\t\tself.input_size = args.max_words # embedding dimention\n",
        "\t\t\n",
        "\t\tself.dropout = nn.Dropout(0.5)\n",
        "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
        "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
        "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
        "\t\tself.fc2 = nn.Linear(257, 1)\n",
        "\t\t\n",
        "\tdef forward(self, x):\n",
        "\t\n",
        "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\t\n",
        "\t\ttorch.nn.init.xavier_normal_(h)\n",
        "\t\ttorch.nn.init.xavier_normal_(c)\n",
        "\n",
        "\t\tout = self.embedding(x)\n",
        "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
        "\t\tout = self.dropout(out)\n",
        "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
        "\t\tout = self.dropout(out)\n",
        "\t\tout = torch.sigmoid(self.fc2(out))\n",
        "\n",
        "\t\treturn out"
      ],
      "metadata": {
        "id": "6Bx-ZPCCWqGs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "\t\n",
        "\tdef __init__(self, args):\n",
        "\t\tself.data = args.file_name\n",
        "\t\tself.max_len = args.max_len\n",
        "\t\tself.max_words = args.max_words\n",
        "\t\tself.test_size = args.test_size\n",
        "\t\t\n",
        "\tdef load_data(self):\n",
        "\t\tdf = pd.read_csv(self.data)\n",
        "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
        "\t\t\n",
        "\t\tX = df['text'].values\n",
        "\t\tY = df['target'].values\n",
        "\t\t\n",
        "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
        "\t\t\n",
        "\tdef prepare_tokens(self):\n",
        "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
        "\t\tself.tokens.fit_on_texts(self.x_train)\n",
        "\n",
        "\tdef sequence_to_token(self, x):\n",
        "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
        "\t\treturn sequence.pad_sequences(sequences, maxlen=self.max_len)"
      ],
      "metadata": {
        "id": "6onUb1KkWuDa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetMaper(Dataset):\n",
        "  '''\n",
        "  Handles batches of dataset\n",
        "  '''\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]\n",
        "    \n",
        "\n",
        "class Execute:\n",
        "  '''\n",
        "  Class for execution. Initializes the preprocessing as well as the \n",
        "  Tweet Classifier model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, args):\n",
        "    self.__init_data__(args)\n",
        "    \n",
        "    self.args = args\n",
        "    self.batch_size = args.batch_size\n",
        "    \n",
        "    self.model = TweetClassifier(args)\n",
        "    \n",
        "  def __init_data__(self, args):\n",
        "    '''\n",
        "    Initialize preprocessing from raw dataset to dataset split into training and testing\n",
        "    Training and test datasets are index strings that refer to tokens\n",
        "    '''\n",
        "    self.preprocessing = Preprocessing(args)\n",
        "    self.preprocessing.load_data()\n",
        "    self.preprocessing.prepare_tokens()\n",
        "\n",
        "    raw_x_train = self.preprocessing.x_train\n",
        "    raw_x_test = self.preprocessing.x_test\n",
        "    \n",
        "    self.y_train = self.preprocessing.y_train\n",
        "    self.y_test = self.preprocessing.y_test\n",
        "\n",
        "    self.x_train = self.preprocessing.sequence_to_token(raw_x_train)\n",
        "    self.x_test = self.preprocessing.sequence_to_token(raw_x_test)\n",
        "    \n",
        "  def train(self):\n",
        "    \n",
        "    training_set = DatasetMaper(self.x_train, self.y_train)\n",
        "    test_set = DatasetMaper(self.x_test, self.y_test)\n",
        "    \n",
        "    self.loader_training = DataLoader(training_set, batch_size=self.batch_size)\n",
        "    self.loader_test = DataLoader(test_set)\n",
        "    \n",
        "    optimizer = optim.RMSprop(self.model.parameters(), lr=args.learning_rate)\n",
        "    for epoch in range(args.epochs):\n",
        "      \n",
        "      predictions = []\n",
        "      \n",
        "      self.model.train()\n",
        "      \n",
        "      for x_batch, y_batch in self.loader_training:\n",
        "        \n",
        "        x = x_batch.type(torch.LongTensor)\n",
        "        y = y_batch.type(torch.FloatTensor)\n",
        "        y_pred = self.model(x)[:,0]\n",
        "        loss = F.binary_cross_entropy(y_pred, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        predictions += list(y_pred.squeeze().detach().numpy())\n",
        "      \n",
        "      test_predictions = self.evaluation()\n",
        "      \n",
        "      train_accuary = self.calculate_accuray(self.y_train, predictions)\n",
        "      test_accuracy = self.calculate_accuray(self.y_test, test_predictions)\n",
        "      \n",
        "      print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
        "      \n",
        "  def evaluation(self):\n",
        "\n",
        "    predictions = []\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      for x_batch, y_batch in self.loader_test:\n",
        "        x = x_batch.type(torch.LongTensor)\n",
        "        y = y_batch.type(torch.FloatTensor)\n",
        "        \n",
        "        y_pred = self.model(x)\n",
        "        predictions += list(y_pred.detach().numpy())\n",
        "        \n",
        "    return predictions\n",
        "      \n",
        "  @staticmethod\n",
        "  def calculate_accuray(grand_truth, predictions):\n",
        "    true_positives = 0\n",
        "    true_negatives = 0\n",
        "    \n",
        "    for true, pred in zip(grand_truth, predictions):\n",
        "      if (pred > 0.5) and (true == 1):\n",
        "        true_positives += 1\n",
        "      elif (pred < 0.5) and (true == 0):\n",
        "        true_negatives += 1\n",
        "      else:\n",
        "        pass\n",
        "        \n",
        "    return (true_positives+true_negatives) / len(grand_truth)"
      ],
      "metadata": {
        "id": "QxqVHGrrWyP0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8pILSmhYKqKk"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self, epochs=10, learning_rate=0.01, hidden_dim=128, \n",
        "           lstm_layers=2, batch_size=64, test_size=0.2, max_len=20,\n",
        "            max_words=1000, file_name='tweets.csv'):\n",
        "    self.epochs=epochs\n",
        "    self.learning_rate=learning_rate\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.lstm_layers=lstm_layers\n",
        "    self.batch_size=batch_size\n",
        "    self.test_size=test_size\n",
        "    self.max_len=max_len\n",
        "    self.max_words=max_words\n",
        "    self.file_name = file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hNqwsvdjJt0J"
      },
      "outputs": [],
      "source": [
        "args = Args()\n",
        "execute = Execute(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execute.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAFnBmM8XwJb",
        "outputId": "56cdb985-b823-42c0-82ef-30aadd72e1b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss: 0.68749, Train accuracy: 0.56782, Test accuracy: 0.57058\n",
            "Epoch: 2, loss: 0.87345, Train accuracy: 0.61593, Test accuracy: 0.57124\n",
            "Epoch: 3, loss: 0.35815, Train accuracy: 0.71675, Test accuracy: 0.76428\n",
            "Epoch: 4, loss: 0.40028, Train accuracy: 0.79343, Test accuracy: 0.76428\n",
            "Epoch: 5, loss: 0.16108, Train accuracy: 0.82824, Test accuracy: 0.78858\n",
            "Epoch: 6, loss: 0.17638, Train accuracy: 0.84844, Test accuracy: 0.77544\n",
            "Epoch: 7, loss: 0.09451, Train accuracy: 0.86897, Test accuracy: 0.78398\n",
            "Epoch: 8, loss: 0.12907, Train accuracy: 0.88654, Test accuracy: 0.78135\n",
            "Epoch: 9, loss: 0.06452, Train accuracy: 0.90509, Test accuracy: 0.78792\n",
            "Epoch: 10, loss: 0.06108, Train accuracy: 0.91560, Test accuracy: 0.77544\n",
            "Epoch: 11, loss: 0.05256, Train accuracy: 0.92627, Test accuracy: 0.78332\n",
            "Epoch: 12, loss: 0.06796, Train accuracy: 0.93251, Test accuracy: 0.76953\n",
            "Epoch: 13, loss: 0.02582, Train accuracy: 0.93826, Test accuracy: 0.76559\n",
            "Epoch: 14, loss: 0.00743, Train accuracy: 0.94516, Test accuracy: 0.76888\n",
            "Epoch: 15, loss: 0.06223, Train accuracy: 0.95057, Test accuracy: 0.78332\n",
            "Epoch: 16, loss: 0.02356, Train accuracy: 0.95255, Test accuracy: 0.77282\n",
            "Epoch: 17, loss: 0.00438, Train accuracy: 0.95419, Test accuracy: 0.77019\n",
            "Epoch: 18, loss: 0.01172, Train accuracy: 0.95846, Test accuracy: 0.77282\n",
            "Epoch: 19, loss: 0.00163, Train accuracy: 0.95813, Test accuracy: 0.78004\n",
            "Epoch: 20, loss: 0.04632, Train accuracy: 0.95796, Test accuracy: 0.77741\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}